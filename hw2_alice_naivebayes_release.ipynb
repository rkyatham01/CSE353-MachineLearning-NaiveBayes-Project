{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alice', 'adventures', 'in', 'wonderland', 'by', 'lewis', 'carroll', 'the', 'millennium', 'fulcrum', 'edition', '3', 'contents', 'chapter', 'i', 'down', 'the', 'rabbit', 'chapter', 'ii', 'the', 'pool', 'of', 'tears', 'chapter']\n",
      "corpus len:  25320\n"
     ]
    }
   ],
   "source": [
    "corpus = []\n",
    "f = open('alice_in_wonderland.txt','r')\n",
    "while(1):\n",
    "    line =  f.readline()\n",
    "    if len(line) == 0: break\n",
    "    corpus.extend(line.split())\n",
    "        \n",
    "f.close()\n",
    "corpus = ' '.join(corpus)\n",
    "\n",
    "def clean_word(word):\n",
    "    word = word.lower()\n",
    "    for punctuation in ['\"',\"'\",'.',',','-','?','!',';',':','â€”','(',')','[',']']:\n",
    "        word = word.split(punctuation)[0]\n",
    "    return word\n",
    "\n",
    "corpus = [clean_word(word) for word in corpus.split()]\n",
    "corpus = [word for word in corpus if len(word) > 0]\n",
    "print(corpus[:25])\n",
    "D = len(corpus)\n",
    "print('corpus len: ',D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word list size (number of distinct words):  2637\n"
     ]
    }
   ],
   "source": [
    "tokenize = {}\n",
    "wordlist = []\n",
    "token = 0\n",
    "for word in corpus:\n",
    "    if word not in tokenize.keys():\n",
    "        tokenize[word] = token\n",
    "        wordlist.append(word)\n",
    "        token += 1\n",
    "    \n",
    "V = len(wordlist)\n",
    "print('word list size (number of distinct words): ', V)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bin how many times a word follows another word\n",
    "def countgram(n):\n",
    "    counts_anygram = np.zeros((V,V))\n",
    "    for i in range(1,len(corpus)):\n",
    "        token_i = tokenize[corpus[i]]\n",
    "        token_im1 = tokenize[corpus[i-1]]\n",
    "        counts_anygram[token_i,token_im1] += 1\n",
    "    return counts_anygram\n",
    "    print(counts_anygram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2500098740076622\n",
      "('was', 0.0007109004739336493)\n",
      "('queen', 0.002764612954186414)\n",
      "('cat', 0.00019747235387045816)\n",
      "('turtle', 0.0022511848341232226)\n"
     ]
    }
   ],
   "source": [
    "#past word as feature\n",
    "\n",
    "posterior_1word = np.zeros((V, V))\n",
    "prior = np.zeros(V)\n",
    "count2_gram = countgram(1)\n",
    "\n",
    "def get_likelihood_2gram(word):\n",
    "    colindx = tokenize[word]\n",
    "    posterior_1word[:, colindx] = (count2_gram[:,colindx] /np.sum(count2_gram[:,colindx]))\n",
    "    prior[colindx] = np.sum(count2_gram[:,colindx]) / len(corpus)\n",
    "    likelihood = posterior_1word[:, colindx] * prior[colindx]\n",
    "    return(likelihood)\n",
    "\n",
    "def pred_2gram(word):\n",
    "    likelihood = get_likelihood_2gram(word)\n",
    "    i = np.argmax(likelihood)\n",
    "    return(wordlist[i], likelihood[i])\n",
    "\n",
    "def checker():\n",
    "    c = 0\n",
    "    for i in range(0, len(corpus)-1):\n",
    "        if corpus[i+1] == pred_2gram(corpus[i])[0]:\n",
    "            c += 1\n",
    "    return c / (len(corpus)-1)\n",
    "\n",
    "print(checker())\n",
    "        \n",
    "print(pred_2gram('alice'))\n",
    "print(pred_2gram('the'))\n",
    "print(pred_2gram('cheshire'))\n",
    "print(pred_2gram('mock'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "pred_thegram() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Rishith\\OneDrive\\Desktop\\hw2 CSE353\\hw2_alice_naivebayes_release.ipynb Cell 6\u001b[0m in \u001b[0;36m<cell line: 20>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Rishith/OneDrive/Desktop/hw2%20CSE353/hw2_alice_naivebayes_release.ipynb#W5sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     i \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margmax(likelihood)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Rishith/OneDrive/Desktop/hw2%20CSE353/hw2_alice_naivebayes_release.ipynb#W5sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m wordlist[i], likelihood[i]\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Rishith/OneDrive/Desktop/hw2%20CSE353/hw2_alice_naivebayes_release.ipynb#W5sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39mprint\u001b[39m(pred_thegram(\u001b[39m'\u001b[39;49m\u001b[39mpack\u001b[39;49m\u001b[39m'\u001b[39;49m,\u001b[39m'\u001b[39;49m\u001b[39mof\u001b[39;49m\u001b[39m'\u001b[39;49m))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Rishith/OneDrive/Desktop/hw2%20CSE353/hw2_alice_naivebayes_release.ipynb#W5sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39mprint\u001b[39m(pred_thegram(\u001b[39m'\u001b[39m\u001b[39mthe\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mmad\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Rishith/OneDrive/Desktop/hw2%20CSE353/hw2_alice_naivebayes_release.ipynb#W5sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39mprint\u001b[39m(pred_thegram(\u001b[39m'\u001b[39m\u001b[39mshe\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mjumped\u001b[39m\u001b[39m'\u001b[39m))\n",
      "\u001b[1;31mTypeError\u001b[0m: pred_thegram() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "#past 2 words as features\n",
    "\n",
    "posterior_2words = np.zeros((V, V))\n",
    "\n",
    "#def get_likelihood_thegram(wordsArr): #passed an array\n",
    "    \n",
    "  #  for i in range(1, len(words)):\n",
    "   #     count = countgram(i-1)\n",
    "    #\n",
    "    #would call countgram, find the # of words that occured, divide etc\n",
    "    # multiply prior and posterier for every word and then return \n",
    "    # the likelihood at the end \n",
    "   # return likelihood\n",
    "    \n",
    "def pred_thegram(wordsArr):\n",
    "\n",
    "    likelihood = get_likelihood_thegram(wordsArr)\n",
    "    i = np.argmax(likelihood)\n",
    "    return wordlist[i], likelihood[i]\n",
    "print(pred_thegram('pack','of'))\n",
    "print(pred_thegram('the','mad'))\n",
    "print(pred_thegram('she','jumped'))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "88279d2366fe020547cde40dd65aa0e3aa662a6ec1f3ca12d88834876c85e1a6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
